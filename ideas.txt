1. model(img1, img2, question) -> answer={'After', 'Before'}
2. BLIP-2 - modal-model {image, text}

Prepare:
1. Model: MMICL-Instructblip <- Instructblip <- Blip2 => Inference
 - non-Instruct -> training: input("I am going to school") -LLM-> output()
 - Instruct -> training: input("Where I am going to?") -LLM-> output("I am going to my school")
    Dataset:{
        "question": "Where I am going to?",
        "answer": "I am going to my school"
    }
2. Datasets: QA -> Done!
    - Train dataset (lỗi thực/ rác, loss người học) | -> Training -> loss >>>>
    - Val | -> Training -> accuracy <<<<< -> public test
    - Test -> Testing/ Benchmarking -> private test
3. Pytorch

Training:
